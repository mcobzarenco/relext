\documentclass[a4paper, 11pt, fleqn]{article}

\usepackage{geometry}
\usepackage{geometry}
 \geometry{
 a4paper,
 total={210mm,297mm},
 left=23mm,
 right=23mm,
 top=30mm,
 bottom=30mm,
 }

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}

\usepackage[pdftex]{graphicx}

\usepackage{subcaption}

\usepackage{fancyhdr}
\pagestyle{fancy}

%%   Document begins here   %%

\newcommand{\btheta}[0]{\boldsymbol{\theta}}
\begin{document}

 \title{Tensor Factorization for Relation Extraction}
\author{Marius Cobzarenco}
\maketitle

\section{Introduction}
% Matrix factorisation methods have been successfully applied to
% relation extraction tasks.
The problem of relation extraction can be formulated as a supervsied
learning problem where given a set $\mathbb{E}$ of entities/objects
and a set $\mathbb{P}$ of $n$-ary predicates the task is to learn new
predicates that may hold between the entities. A predicate (or
relation, both terms will be used interchangeably) is defined as a
boolean function $r:\mathbb{E}^n \rightarrow \{0, 1\}$.

\[examples of relations\]

For ease of comparison with previous work, we will focus on the case
of binary relations. However, all models discussed below are trivially
applicable to predicates of arbitrary arrity. Let us assume a training
set that consists of $ N_E \equiv \vert\mathbb{E}\vert$ entities, $N_R
\equiv \vert\mathbb{P}\vert$ relations and $N$ tuples
\begin{align}
X \equiv \{(i_n, j_n, k_n, r_n)\}_{n\in\overline{1..N}}
\end{align}
which are interpreted as asserting that relation $R_{k_n}$ either
holds ($r_n = 1$) or does not hold ($r_n = 0$) between entities
$e_{i_n}$ and $e_{j_n}$.
\begin{align}
R_{k_n}(e_{i_n}, e_{j_n}) &= r_n
\end{align}
The training tuples have the constraints that for $n\in{1..N}$ all
$(i_n, j_n, k_n)$ are distinct and
\begin{align}
1 \leq i_n \leq N_E, \quad 1 \leq j_n \leq N_E, \quad
1 \leq k_n \leq N_R,\quad r_n \in \left\{0, 1\right\}
\end{align}
Note that most natural language relations are not symmetric.

\section{Tensor Decomposition}

\subsection{Log-Bilinear Model}
We propose modelling each entity $e_i$ as a real valued vector ${\bf
e}_i \in \mathbb{R}^D$ and each relation $R_k$ as a matrix ${\bf R}_k
\in \mathbb{R}^{D \times D}$. Given entities $e_i$ and $e_j$
the probability of a relation $R_k$ being true is modelled as
\begin{align}
  &p\left(R_k(e_i, e_j) = 1\vert \btheta\right) = \sigma\left({\bf
      e}'_i{\bf R}_k{\bf e}_j\right) \equiv \sigma\left({\bf
      e}'_i{\bf U}_k{\bf V}_k{\bf e}_j\right),
&\sigma(x) \equiv \left(1 + \exp(-x)\right)^{-1}
\end{align}
\noindent To make the embeddings interpretable and reguralize the
objective, ${\bf R}_k$ was constrained to be low rank ${\bf R}_k
\equiv {\bf U}_k{\bf V}_k$ for
some $ {\bf U}_k \in \mathbb{R}^{D
  \times \rho}$ and $ {\bf V}_k \in \mathbb{R}^{\rho \times D}$ with
$\rho \equiv \text{rank}({\bf R}_k) < D$. The parameters $\btheta$ are
the embeddings for entities and relations
\begin{align}
\btheta \equiv \{{\bf e}_i \vert i \in \overline{1..N_E}\} \cup
 \{{\bf U}_k, {\bf V}_k \vert k \in \overline{1..N_R}\}
\end{align}
An independent Gaussian prior was placed on the coefficients of the
embeddings
\begin{align}
\forall i \in \overline{1..N_E}.\;\; {\bf e}_i \sim \mathcal{N}({\bf 0},
\sigma^2); \quad \forall k \in \overline{1..N_R}.\;\; {\bf U}_k[:],
{\bf V}_k[:] \sim
\mathcal{N}({\bf 0}, \sigma^2)
\end{align}
Given a dataset assumed assumed to be i.i.d., the posterior
distribution over $\btheta$ up to the normalisation constant is
\begin{align}
  \label{eqn:prob_dataset}
  p(\btheta\vert X) &\propto p(X\vert \btheta)p(\btheta) \nonumber \\
&= p\left(\left\{R_{k_n}(e_{i_n}, e_{j_n})=r_n\right\}_{n\in\overline{1..N}}\vert
    \btheta\right) p(\btheta)\nonumber \\
  &= \prod_{n=1}^{N}\left(1 - \sigma({\bf e}'_{i_n}{\bf R}_{k_n}{\bf
     e}_{j_n})\right)^{1 - r_n}\sigma({\bf e}'_{i_n}{\bf R}_{k_n}{\bf
      e}_{j_n})^{r_n} \prod_{i=1}^{N_E} (2\pi\sigma^2)^{-\frac{D}{2}}
    \exp\left(-\frac{{\bf e}_i'{\bf e}_i}{2\sigma^2}\right) \nonumber \\
&\times \prod_{k=1}^{N_R} (2\pi\sigma^2)^{-D\rho}
    \exp\left(-\frac{\text{Tr}\left({\bf U}_k'{\bf U}_k + {\bf V}_k{\bf V}_k'\right)}{2\sigma^2}\right)
\end{align}
We propose fitting this model by finding the maximum a posteriori
(MAP) estimate $\btheta^* \equiv
\operatorname{argmax} L(\btheta)$,  with $L(\btheta)
\equiv \log p(X\vert \btheta)p(\btheta)$
\begin{align}
  \label{eqn:loglike}
  L(\btheta) &= \sum_{n=1}^{N}\left[ (1 - r_n) \log \left(1 -
    \sigma({\bf e}'_{i_n}{\bf R}_{k_n}{\bf e}_{j_n})\right) + r_n \log
  \sigma({\bf e}'_{i_n}{\bf R}_{k_n}{\bf e}_{j_n})\right] \nonumber \\
  &\quad - \frac{1}{2\sigma^2}\left[ \sum_{i=1}^{N_E} {\bf e}_i'{\bf e}_i
  +\sum_{k=1}^{N_R}
  \text{Tr}\left({\bf U}_k'{\bf U}_k + {\bf V}_k{\bf V}_k'\right)\right]
  - \frac{DN_E + 2D\rho N_R}{2}\log(2\pi\sigma^2)
\end{align}
The first term in the first summation in $L(\btheta)$ can be rewritten as
\begin{align}
(1 - r_n) \log \left(1 -
    \sigma({\bf e}'_{i_n}{\bf R}_{k_n}{\bf e}_{j_n})\right) =
  (r_n - 1)\left( {\bf e}'_{i_n}{\bf R}_{k_n}{\bf e}_{j_n} +
  \log(1 + \exp(-{\bf e}'_{i_n}{\bf R}_{k_n}{\bf e}_{j_n})) \right)
\end{align}
This helps with numerical stability when the sigmoid would otherwise
saturate. The function $log(1 + x)$ is often implemtened in numerical
packages as \texttt{log1p} such that it is accurate for small $x$
where $1 + x = 1$ in floating point arithmetic. $L$'s partial
derivatives w.r.t. $\btheta$ can be computed as
\begin{align}
  \label{eqn:objective}
  \frac{\partial L}{\partial {\bf U}_k} &=
  \sum_{n=1}^{N}\delta_{k,k_n}\left[(r_n - 1)\sigma({\bf
      e}'_{i_n}{\bf R}_{k}{\bf e}_{j_n}) + r_n \left( 1 - \sigma({\bf
        e}'_{i_n}{\bf R}_{k}{\bf e}_{j_n}) \right)\right]
  {\bf e}_{i_n}{\bf e}'_{j_n}{\bf V}'_k- \frac{1}{\sigma^{2}}{\bf U}_k \\
  \frac{\partial L}{\partial {\bf V}_k} &=
  \sum_{n=1}^{N}\delta_{k,k_n}\left[(r_n - 1)\sigma({\bf
      e}'_{i_n}{\bf R}_{k}{\bf e}_{j_n}) + r_n \left( 1 - \sigma({\bf
        e}'_{i_n}{\bf R}_{k}{\bf e}_{j_n}) \right)\right]
  {\bf U}'_k{\bf e}_{i_n}{\bf e}'_{j_n} - \frac{1}{\sigma^{2}}{\bf V}_k \\
  \frac{\partial L}{\partial {\bf e}_i} &=
  \sum_{n=1}^{N}\left[(r_n - 1)\sigma({\bf
      e}'_{i_n}{\bf R}_{k_n}{\bf e}_{j_n}) + r_n \left( 1 - \sigma({\bf
        e}'_{i_n}{\bf R}_{k_n}{\bf e}_{j_n}) \right)\right] \left( \delta_{i,i_n}{\bf R}_{k_n}
    {\bf e}_{j_n} + \delta_{i,j_n}{\bf R}'_{k_n} {\bf
      e}_{i_n}\right)\nonumber \\
  &\qquad - \frac{1}{\sigma^2} {\bf e}_i
\end{align}

\section{Bayesian Personalised Ranking}
% In particular, we are building on \cite{riedel13relation}


% \section{Background}

% Noise-contrastive estimation (NCE) is a recent approximate point-wise
% estimation principle for probabilistic models first introduced by
% Hyv{\"a}rinen et al in \cite{gutmann2010noise}. In particular, the
% method can be applied to unnormalized probabilistic models, allowing
% to jointly optimise the normalisation constant and the parameters of
% the model simultaneously. This is achieved through introducing an
% alternative to the maximum likelihood estimator (MLE) and analysing
% its asymptotic properties in large samples. In contrast to ML
% estimation, the normalisation constant can be treated as any other
% parameter. The underlying optimisation problem is unconstrained and
% yields a normalised model. As the contribution of the theory is a new
% estimator shown to have desirable asymptotic properties under repeated
% sampling, the analysis of its statistical properties is necessarily
% frequentist.

% \noindent Let $X = \{ {\bf x}_1, ..., {\bf x}_N \}$ be a sample
% dataset with ${\bf x}_i \in \mathcal{R}^D, \forall i \in
% \overline{1..N}$. The data is assumed to come from an unknown sampling
% distribution $p_d$ and it is modelled by a family of probability
% distributions $\mathcal{F} \equiv {p_m(\cdot\vert {\boldsymbol
%     \theta})}$ parameterized by ${\bf \theta}$. Moreover, it is
% assumed $\mathcal{F}$ is flexible enough: there exists some parameter
% ${\boldsymbol \theta}^*$ such that $p_d(\cdot) =
% p_m(\cdot\vert{\boldsymbol \theta^*})$. In this context, the learning
% problem is finding $\boldsymbol \theta^*$ given the training dataset
% $X$.

% \noindent The theory of noise-contrastive estimation is motivated by
% characterising the properties of $X$ in relation to a generated
% reference \emph{noise} dataset $Y$ with known distribution $p_n$. The
% probability densities of the noise and the training data are related
% by the ratio $p_d/p_n$. The comparison between the two datasets is
% achieved by training a logistic regression classifier to discriminate
% between noise and the dataset $X$. Assume $M$ noise samples are
% generated, such that $Y = \{ {\bf y}_1, ..., {\bf y}_M \}$ and let $Z
% \equiv X \cup Y = \{ {\bf z}_1, ..., {\bf z}_{N + M} \}$. For each
% datapoint ${\bf z}_i$, we assign a class label $c_i = 1$ iff ${\bf
%   z}_i \in X$ and $c_i = 0$ iff ${\bf z}_i \in Y$. As the sampling
% distribution $p_d$ is unknown, $p({\bf z}\vert c=1)$ is taken to be
% $p_m({\bf z}\vert {\boldsymbol \theta})$ and thus:
% \begin{align*}
%   p({\bf z} \vert c = 0, {\boldsymbol \theta}) &= p_n({\bf z})
%   &p(c = 0) = \frac{M}{M + N}\\
%   p({\bf z} \vert c = 1, {\boldsymbol \theta}) &= p_m({\bf z}\vert
%   {\boldsymbol \theta})
%   &p(c = 1) = \frac{N}{M + N}
% \end{align*}
% \noindent Using Bayes theorem and letting $\sigma(x) \equiv 1 / (1 +
% \exp(-x))$ denote the logistic function, the posterior class
% probability for a noise sample can be written as
% \begin{align}
%   p(c = 0 \vert {\bf z}, {\boldsymbol \theta}) &= \frac{p( {\bf z}
%     \vert c = 0, {\boldsymbol \theta})p(c = 0)} {p( {\bf z} \vert c =
%     0, {\boldsymbol \theta})p(c = 0) + p( {\bf z}
%     \vert c = 1, {\boldsymbol \theta})p(c = 1)} \notag \\
%   &= \frac{Mp_n({\bf z})}{Mp_n({\bf z}) + Np_m({\bf z}\vert
%     {\boldsymbol \theta})} = \frac{\alpha p_n({\bf z})}{\alpha
%     p_n({\bf z})
%     + p_m({\bf z}\vert {\boldsymbol \theta})} \notag \\
%   &= 1 - \sigma\left(\log \frac{p_m({\bf z}\vert {\boldsymbol
%         \theta})}{\alpha p_n({\bf z})} \right)
% \end{align}
% Where $\alpha \equiv M / N$ is the ratio between the number of noise
% samples and the number of training datapoints. In the last line above
% the relation $\sigma(-x) = 1 - \sigma(x)$ was used. Similarly, the
% posterior class probability for a training datapoint is
% \begin{align}
%   p(c = 1 \vert {\bf z}, {\boldsymbol \theta}) = \frac{p_m({\bf
%       z}\vert {\boldsymbol \theta})}{\alpha p_n({\bf z}) + p_m({\bf
%       z}\vert {\boldsymbol \theta})} = \sigma\left(\log \frac{p_m({\bf
%         z}\vert {\boldsymbol \theta})}{\alpha p_n({\bf z})} \right)
% \end{align}
% \noindent The quantities can be written more succinctly by introducing
% a function $h$
% \begin{align}
%   h({\bf z}, \alpha, {\boldsymbol \theta}) \equiv \sigma\left(\log
%     \frac{p_m({\bf z}\vert {\boldsymbol \theta})}{\alpha p_n({\bf z})}
%   \right)
% \end{align}
% \noindent such that
% \begin{align}
% &p(c = 0 \vert {\bf z}, {\boldsymbol \theta}) = 1 - h({\bf z}, \alpha, {\boldsymbol \theta})
% &p(c = 1 \vert {\bf z}, {\boldsymbol \theta}) = h({\bf z}, \alpha, {\boldsymbol \theta})
% \end{align}

\end{document}
